{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0+vZ9gl7Vf0JdRWXC3Hec",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham-ai/ChatGPT/blob/main/Sentimental_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAO84IpbbqrD",
        "outputId": "9058cd6a-0ffe-4230-f1a7-57c64f38d53a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /root/.local/lib/python3.10/site-packages (0.0.272)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /root/.local/lib/python3.10/site-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.11.1)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /root/.local/lib/python3.10/site-packages (from langchain) (0.0.26)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.2.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /root/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /root/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=2.11.1->langchain) (1.60.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=2.11.1->langchain) (3.20.3)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=2.11.1->langchain) (2.17.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.11.1->langchain) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.11.1->langchain) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.11.1->langchain) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.11.1->langchain) (4.9)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /root/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.11.1->langchain) (0.5.0)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.22.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: sentencepiece in /root/.local/lib/python3.10/site-packages (0.1.99)\n",
            "Requirement already satisfied: einops in /root/.local/lib/python3.10/site-packages (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "# !pip install \"langchain>=0.0.218\" openai -qqq\n",
        "!pip install \"langchain\"  --user\n",
        "\n",
        "# assert langchain.__version__ >= \"0.0.218\", \"Please ensure you are using LangChain v0.0.188 or higher\"\n",
        "import os\n",
        "!pip install transformers[torch]\n",
        "!pip install sentencepiece --user\n",
        "!pip install einops --user\n",
        "os.environ[\"Langchain\"] = \"langchain-testing\"\n",
        "!pip install vaderSentiment --user"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/google/flan-t5-base\n",
        "# !git clone https://huggingface.co/google/flan-t5-xxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLaKhIfhd2-Z",
        "outputId": "9aaf47dd-6274-47e3-d326-a00be37175c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'flan-t5-base'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 58 (delta 0), reused 0 (delta 0), pack-reused 55\u001b[K\n",
            "Unpacking objects: 100% (58/58), 621.24 KiB | 2.38 MiB/s, done.\n",
            "Filtering content: 100% (5/5), 3.87 GiB | 48.88 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import PromptTemplate\n",
        "import datetime\n",
        "import os\n",
        "import torch\n",
        "# turn on wandb logging for langchain\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_id = \"flan-t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"flan-t5-base\")\n",
        "\n",
        "# model = T5ForConditionalGeneration.from_pretrained(model_id)\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\", torch_dtype=torch.float16).to(device)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model, tokenizer=tokenizer, max_length=512\n",
        ")\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=API_KEY)\n",
        "\n",
        "# first step in chain\n",
        "start_time_ms = datetime.datetime.now().timestamp() * 1000\n",
        "\n",
        "template='''You analyse the sentiment of the text provided.\n",
        "The sentiment can be positive, negative or neutral.\n",
        "{text}\n",
        "'''\n",
        "prompt_1 = PromptTemplate(\n",
        "\n",
        "input_variables=[\"text\"],\n",
        "\n",
        "template=template)\n",
        "\n",
        "chain_1 = LLMChain(llm = local_llm, prompt = prompt_1, verbose=False,output_key=\"sentiment\")\n",
        "\n",
        "# second step in chain\n",
        "\n",
        "prompt_2 = PromptTemplate(\n",
        "\n",
        "input_variables=[\"text\"],\n",
        "\n",
        "template='''tell me the percentage of politeness in the text provided. {text} ''')\n",
        "\n",
        "chain_2 = LLMChain(llm = local_llm, prompt = prompt_2, verbose=False,output_key=\"politeness score\")\n",
        "\n",
        "# Third step in chain\n",
        "\n",
        "prompt_3 = PromptTemplate(\n",
        "\n",
        "input_variables=[\"text\"],\n",
        "\n",
        "template='''tell me only negative words found in the text provide. {text} ''')\n",
        "\n",
        "chain_3 = LLMChain(llm = local_llm, prompt = prompt_3, verbose=False,output_key=\"negative words\")\n",
        "# fourth step in chain\n",
        "\n",
        "prompt_4 = PromptTemplate(\n",
        "\n",
        "input_variables=[\"text\"],\n",
        "\n",
        "template='''tell me only positive words found in the text provide. {text}''')\n",
        "\n",
        "chain_4 = LLMChain(llm = local_llm, prompt = prompt_4, verbose=False,output_key=\"positive words\")\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_1,chain_2,chain_3,chain_4],\n",
        "    input_variables=[\"text\"],\n",
        "    # Here we return multiple variables\n",
        "    output_variables=[\"sentiment\",\"politeness score\",\"negative words\",\"positive words\"],\n",
        "    verbose=True)\n",
        "\n",
        "\n",
        "end_time_ms = round(datetime.datetime.now().timestamp() * 1000)  # logged in milliseconds\n",
        "# text=\" The battery is long lasting\"\n",
        "text=[\"go to hell\",\"the battery is long lasting\",\"hey how are you doing\"]\n",
        "for sentence in text:\n",
        "\n",
        "  print(overall_chain({\"text\":sentence}))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXl_LylycBbV",
        "outputId": "35804bcc-2532-45a9-a202-58d136d63139"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'text': 'go to hell', 'sentiment': 'negative', 'politeness score': '0', 'negative words': 'go to hell', 'positive words': 'go to hell'}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'text': 'the battery is long lasting', 'sentiment': 'positive', 'politeness score': '14%', 'negative words': 'battery', 'positive words': 'battery'}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'text': 'hey how are you doing', 'sentiment': 'positive', 'politeness score': '80%', 'negative words': 'how are you doing', 'positive words': 'hey'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH4gkFrY_tu0",
        "outputId": "669bf83f-94ba-4671-822d-b1d2d0fb3e7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2023.7.22)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import SentimentIntensityAnalyzer class\n",
        "# from vaderSentiment.vaderSentiment module.\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# function to print sentiments\n",
        "# of the sentence.\n",
        "def sentiment_scores(sentence):\n",
        "\n",
        "\t# Create a SentimentIntensityAnalyzer object.\n",
        "\tsid_obj = SentimentIntensityAnalyzer()\n",
        "\n",
        "\t# polarity_scores method of SentimentIntensityAnalyzer\n",
        "\t# object gives a sentiment dictionary.\n",
        "\t# which contains pos, neg, neu, and compound scores.\n",
        "\tsentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "\n",
        "\tprint(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
        "\tprint(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
        "\tprint(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
        "\tprint(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
        "\n",
        "\tprint(\"Sentence Overall Rated As\", end = \" \")\n",
        "\n",
        "\t# decide sentiment as positive, negative and neutral\n",
        "\tif sentiment_dict['compound'] >= 0.05 :\n",
        "\t\tprint(\"Positive\")\n",
        "\n",
        "\telif sentiment_dict['compound'] <= - 0.05 :\n",
        "\t\tprint(\"Negative\")\n",
        "\n",
        "\telse :\n",
        "\t\tprint(\"Neutral\")\n",
        "\n",
        "\n",
        "\n",
        "# Driver code\n",
        "if __name__ == \"__main__\" :\n",
        "  text=[\"go to hell\",\"the battery is long lasting\",\"hey how are you doing\"]\n",
        "  for i in text:\n",
        "    print(i)\n",
        "    sentiment_scores(i)\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkc4vInB_LxR",
        "outputId": "be913f0f-032f-4dbc-e93c-5b3082fe9e50"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go to hell\n",
            "Overall sentiment dictionary is :  {'neg': 0.697, 'neu': 0.303, 'pos': 0.0, 'compound': -0.6808}\n",
            "sentence was rated as  69.69999999999999 % Negative\n",
            "sentence was rated as  30.3 % Neutral\n",
            "sentence was rated as  0.0 % Positive\n",
            "Sentence Overall Rated As Negative\n",
            "the battery is long lasting\n",
            "Overall sentiment dictionary is :  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "sentence was rated as  0.0 % Negative\n",
            "sentence was rated as  100.0 % Neutral\n",
            "sentence was rated as  0.0 % Positive\n",
            "Sentence Overall Rated As Neutral\n",
            "hey how are you doing\n",
            "Overall sentiment dictionary is :  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "sentence was rated as  0.0 % Negative\n",
            "sentence was rated as  100.0 % Neutral\n",
            "sentence was rated as  0.0 % Positive\n",
            "Sentence Overall Rated As Neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "RH-KuPm6BHe7",
        "outputId": "3f44abbb-0f7c-4f82-dbc7-430d4f9c4e70"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hey how are you doing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.base import LLM\n",
        "import torch\n",
        "\n",
        "class CustomLLM(LLM):\n",
        "    model_name = \"upstage/llama-65b-instruct\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        load_in_8bit=True,\n",
        "        rope_scaling={\"type\": \"dynamic\", \"factor\": 2}  # allows handling of longer inputs\n",
        "    )\n",
        "\n",
        "    def _call(self, prompt, stop=None, **kwargs):\n",
        "        prompt = f\"### User:\\n{prompt}\\n\\n### Assistant:\\n\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        output = self.model.generate(**inputs, streamer=self.streamer, use_cache=True, max_new_tokens=float('inf'))\n",
        "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self):\n",
        "        return {\"name_of_model\": self.model_name}\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self):\n",
        "        return \"custom\"\n",
        "\n",
        "\n",
        "llm_predictor = LLMPredictor(llm=CustomLLM())"
      ],
      "metadata": {
        "id": "2jDN-hlAfblQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}